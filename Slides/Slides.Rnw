\documentclass{beamer}
\usepackage{graphicx, epsfig, amsmath, amssymb}
\usepackage{wrapfig}
%\usepackage{sidecap}
\usetheme{AnnArbor}
\usecolortheme{beaver}
%\usetheme{Boadilla}
%\usetheme{Warsaw}
%\usepackage[table]{xcolor}
%\definecolor{lightgray}{gray}{0.9}


\setbeamertemplate{footline}
{
\leavevmode
\hbox{\begin{beamercolorbox}[wd=0.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm plus1fill,rightskip=.3cm]{author in head/foot}
\usebeamerfont{author in head/foot}\insertshortauthor
\end{beamercolorbox}%
\begin{beamercolorbox}[wd=0.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm,rightskip=.3cm plus1fil]{title in head/foot}%
\usebeamerfont{title in head/foot}\insertshorttitle\hspace*{3em}
\insertframenumber{} / \inserttotalframenumber\hspace*{1ex}
\end{beamercolorbox}}%
\vskip0pt%
}

\makeatletter
\setbeamertemplate{navigation symbols}{}

\title[CC Defense]{Applying Polynomial Regression to Dyadic Data}
\author{Nehemias Ulloa}
\institute{\Large{Department of Statistics\\ Iowa State University}}
\date{\today}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%
%%%%% Title Page %%%%%
%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\titlepage
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%
%%%%% Outline %%%%%
%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item Intro and Motivation
\item Model
\item Recreate Dr. Phillips' Application
\item Our Application
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Intro/Motivation %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Introduction}
Hypotheses:
\begin{itemize}
  \item Family researchers comparing the attitudes, behaviors, and opinions of pairs
  \item Collect Dyadic data
    \begin{itemize}
      \item Inter-individual reporting
      \item Intra-individual reporting
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Motivation}
\begin{itemize}
  \item Difference scores used to analyze dyadic data
  \item Difference scores allow to see how well they ``fit'' together
  \item Common types: algebraic, absolute, and squared difference
\end{itemize}

\begin{align}
Z &= \beta_0 + \beta_1 (X - Y) + \epsilon \label{eq:diffscore} \\
Z &= \beta_0 + \beta_1 |X - Y| + \epsilon \label{eq:absdiffscore} \\
Z &= \beta_0 + \beta_1 (X - Y)^2 + \epsilon \label{eq:squarediffscore}
\end{align}
\end{frame}


\begin{frame}
\frametitle{Motivation}
\begin{itemize}
  \item Many methodological issues with Difference Scores
  \begin{enumerate}
    \item Difficult to identify the underlying mechanism
    \item Problems with underlying assumptions
  \end{enumerate}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Motivation}
\begin{itemize}
  \item Any alternatives?
  \item Polynomial Regression \\ e.g.
  \begin{align}
    Z &= \beta_0 + \beta_1 X + \beta_2 Y + \beta_3 X^2 + \beta_4 Y^2 + \beta_5 XY + \epsilon
  \end{align}
  \item Take the Squared Difference and expand it
    \begin{align}
    (X-Y)^2 &= X^2 + Y^2 -2XY
  \end{align}
  \item Expand on the ideas from Simple Linear Regression
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% Model %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{The Model}
\begin{itemize}
  \item Theoretical model:
    \begin{align}
    Z &= \beta_0 + \beta_1 X + \beta_2 Y + \beta_3 X^2 + \beta_4 Y^2 + \beta_5 XY + \epsilon
  \end{align}
  \item Fitted model:
    \begin{align}
    \hat{z} &= b_{0} + b_{1}x + b_{2}y + b_3 x^2 + b_4 xy + b_5 y^2 \label{fittedmod} 
    \end{align}
  \item Fitted model in matrix notation:
\begin{align}
\hat{z} &= b_0 + {\bf d}^{'} {\bf b} + {\bf d}^{'} {\bf B}{\bf d}
\end{align}
where
\[ {\bf d} = \left[ \begin{array}{cc}
x  \\
y 
\end{array} \right]  \quad
%
{\bf b} = \left[ \begin{array}{cc}
b_1  \\
b_2 
\end{array} \right]  \quad
%
{\bf B} = \left[ \begin{array}{cc}
b_3 & b_4 /2 \\
b_4 /2 & b_5
\end{array} \right]
\]
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Model}
We can fit a model like this in $\verb|R|$ using the $\verb|lm()|$ function:
<<lm_ex, echo=TRUE, eval=FALSE>>=
# Z - the response variable
# X - the first explanatory variable
# Y - the second explanatory variable
QuadFit <- lm(z ~ x + y + I(x^2) + I(x*y) + I(y^2), data=d)
@
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% Stationarity Points %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Stationarity Points: How \& Why?}
\begin{itemize}
  \item What are they?
  \begin{itemize}
    \item Points where the slope is zero no matter which direction you take the derivative
  \end{itemize}
  \item Values of our explanatory variables provide the ``best'' fit for the response \\
  \item How do you derive the stationary points?
  \begin{enumerate}
    \item Take the derivatives of Equation \ref{fittedmod} with respect to $x$ and $y$
    \item Set the derivatives equal to zero
    \item Solve for $x$ and $y$ in terms of {\bf b} and {\bf B} to find the stationarity points
    \item Refer to these points as $x_0$ and $y_0$
  \end{enumerate}
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Stationarity Points}
\begin{align}
x_0 &= \frac{b_2 b_4 - 2 b_5 b_1}{4 b_5 b_3 - b_4^2} \label{eq:xstatpt} \\
y_0 &= \frac{b_1 b_4 + 2 b_2 b_3}{4 b_5 b_3 - b_4^2} \label{eq:ystatpt}
\end{align}

These points represent the values in our predictors that will optimize our response (minimum or maximum depending on the surface shape).

<<stat_pts_function, echo=FALSE, eval=TRUE>>=
statpts <- function(lm){
  b0 <- as.numeric(coef(lm)[1])
  b1 <- as.numeric(coef(lm)[2])
  b2 <- as.numeric(coef(lm)[3])
  b3 <- as.numeric(coef(lm)[4])
  b4 <- as.numeric(coef(lm)[5])
  b5 <- as.numeric(coef(lm)[6])
  
  # Stationary Pts. using the formulas in Eq 10 & 11
  x0  <- (b2*b4 - 2*b1*b5)/(4*b3*b5 - b4^2)
  y0  <- (b1*b4 - 2*b2*b3)/(4*b3*b5 - b4^2)
  
  # Output
  out <- matrix(c(x0, y0), ncol=2)
  colnames(out) <- c("x0","y0")
  out <- data.frame(x0=x0, y0=y0)
  return(out)
}
@

<<stat_pts_function1, echo=TRUE, eval=FALSE>>=
statpts <- function(lm){
  # Stationary Pts.
  x0  <- (b2*b4 - 2*b1*b5)/(4*b3*b5 - b4^2)
  y0  <- (b1*b4 - 2*b2*b3)/(4*b3*b5 - b4^2)
  # Output
  out <- matrix(c(x0, y0), ncol=2)
  colnames(out) <- c("x0","y0")
  out <- data.frame(x0=x0, y0=y0)
  return(out)
}
@

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Pred Response @ Stationary Pts %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]
\frametitle{Predicted Response at Stationarity Points}
We then get the maximum (or minimum) predicted reponse ($\hat{z}_0$) by plugging in the Stationary Points to Eq \ref{fittedmod}:
\begin{align}
\hat{z}_0 &= b_{0} + b_{1}x_0 + b_{2}y_0 + b_3 x_0^2 + b_4 x_0 y_0 + b_5 y_0^2
\end{align}

<<pred_stat_pts, echo=TRUE, eval=FALSE>>=
x0 <- statpts(lm)$x0
y0 <- statpts(lm)$y0
predstatpts <- coef(lm)[1] + coef(lm)[2]*x0 + coef(lm)[3]*y0 + coef(lm)[4]*x0^2 + coef(lm)[5]*x0*y0 + coef(lm)[6]*y0^2
@

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Principal Axes %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]
\frametitle{Principal Axes}
\begin{itemize}
  \item Measure the amount of ``bend'' in two directions at the stationary points
  \item Make interpretations of the model easier by rotating the axes by removing all the cross-product terms (Ex: PCA)
\end{itemize}


In the Creative Componant, 
\begin{itemize}
  \item Brought together a complete picture of how to derive the Principal Axes. 
  \begin{itemize}
    \item Khuri and Cornell (1996) in {\it Response Surfaces: Designs and Analyses} lay out some pieces of how derive them but never completely laid out the complete process
  \end{itemize}
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Principal Axes}

Basic idea in their derivation:
\begin{itemize}
  \item Derive canonical equations to get surface in what is known as canonical form
  \item Transform canonical equations back onto original variables
  \item These are the Principal Axes
\end{itemize}

\begin{align}
\quad        y &= -P_{21}x + P_{20} \label{eq:principalaxes1}
\end{align}

where $P_{20} = y_0 + P_{21} x_0$ and $P_{21} = \frac{b_5 - b_3 - \sqrt{(b_5 - b_3)^2 + b_4^2}}{b_4}$. \\


Using similar algebra we can find the second principal axes:
\begin{align}
\quad        y &= -P_{11}x + P_{10} \label{eq:principalaxes2}
\end{align}
where $P_{10} = y_0 + P_{11} x_0$ and $P_{11} = \frac{b_5 - b_3 + \sqrt{(b_5 - b_3)^2 + b_4^2}}{b_4}$.\\

\end{frame}


\begin{frame}[fragile]
\frametitle{Principal Axes}

Interpretations:
\begin{enumerate}
  \item In a concave surface:
  \begin{itemize}
    \item First principal axis (Eq \ref{eq:principalaxes1}): lowest downward curvature where two explanatory variables create a maximized surface that decreases the least
    \item Second principal axis (Eq \ref{eq:principalaxes2}): greatest downward curvature where two explanatory variables create the greatest decrease in the response
  \end{itemize}
  \item In a convex surface:
  \begin{itemize}
    \item Flip the interpretations of the two Principal Axes
  \end{itemize}
\end{enumerate}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Re-creation of Previous Results %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]
\frametitle{Re-creation}

An example of this methodology being applied is found in Phillips et al. (2012), {\it Congruence research in behavioral medicine: methodological review and demonstration of alternative methodology}.\\

Interested in seeing what created good situations in which patients followed through with their doctors orders

The variables of interest in this example are:
\begin{align*}
Z_{1,i} &- \text{patient-reported adherence a month after the dr. visit for patient } i  \\
Z_{2,i} &- \text{physician's percieved agreement with patient } i  \text{ on the illness treatment} \\
X_{i}   &- \text{physician's rating of patient } i\text{'s health} \\
Y_{i}   &- \text{physician's estimate of how patient } i \text{ would rate of their own health} \\
\end{align*}


\end{frame}















\end{document}