\documentclass{beamer}
\usepackage{graphicx, epsfig, amsmath, amssymb}
\usepackage{wrapfig}
%\usepackage{sidecap}
\usetheme{AnnArbor}
\usecolortheme{beaver}
%\usetheme{Boadilla}
%\usetheme{Warsaw}
%\usepackage[table]{xcolor}
%\definecolor{lightgray}{gray}{0.9}


\setbeamertemplate{footline}
{
\leavevmode
\hbox{\begin{beamercolorbox}[wd=0.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm plus1fill,rightskip=.3cm]{author in head/foot}
\usebeamerfont{author in head/foot}\insertshortauthor
\end{beamercolorbox}%
\begin{beamercolorbox}[wd=0.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm,rightskip=.3cm plus1fil]{title in head/foot}%
\usebeamerfont{title in head/foot}\insertshorttitle\hspace*{3em}
\insertframenumber{} / \inserttotalframenumber\hspace*{1ex}
\end{beamercolorbox}}%
\vskip0pt%
}

\makeatletter
\setbeamertemplate{navigation symbols}{}

\title[CC Defense]{Applying Polynomial Regression to Dyadic Data}
\author{Nehemias Ulloa}
\institute{\Large{Department of Statistics\\ Iowa State University}}
\date{\today}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%
%%%%% Title Page %%%%%
%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\titlepage
<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(foreign)
library(scatterplot3d)
library(car)
library(ggplot2)
library(reshape2)
library(gridExtra)
library(xtable)
set.seed(808)
#library(plotly)
#Sys.setenv("plotly_username" = "nulloa1")
#Sys.setenv("plotly_api_key" = "xfcqdg418r")
#p <- plotly(username="nulloa1", key="xfcqdg418r")
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%
%%%%% Outline %%%%%
%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item Intro and Motivation
\item Model
\item Re-creation of Previous Results
\item Our Application
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Intro/Motivation %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Introduction}
\begin{itemize}
  \item Family researchers comparing the attitudes, behaviors, and opinions of pairs
  \item Dyadic data
    \begin{itemize}
      \item Inter-individual reporting e.g. comparison of husband's report of his support toward his wife to the wife's report of husband's support
      \item Intra-individual reporting e.g. comparison of an observer's report of husband's hostility to an observer's report of the wife's hostility
    \end{itemize}
\end{itemize}

Goal:
\begin{itemize}
  \item Compare observed reports of a dyadic structure to self-reported data
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Motivation}
\begin{itemize}
  \item Difference scores most common method used to analyze dyadic data
  \item Difference scores allow researchers to see how well 2 variables ``fit'' together
  \item Common types: algebraic, absolute, and squared difference
\end{itemize}

\begin{align}
Z &= \beta_0 + \beta_1 (X - Y) + \epsilon \label{eq:diffscore} \\
Z &= \beta_0 + \beta_1 |X - Y| + \epsilon \label{eq:absdiffscore} \\
Z &= \beta_0 + \beta_1 (X - Y)^2 + \epsilon \label{eq:squarediffscore}
\end{align}
\end{frame}


\begin{frame}
\frametitle{Motivation}
\begin{itemize}
  \item Any alternatives?
  \item Polynomial Regression \\ e.g.
  \begin{align}
    Z &= \beta_0 + \beta_1 X + \beta_2 Y + \beta_3 X^2 + \beta_4 Y^2 + \beta_5 XY + \epsilon
  \end{align}
  \item Take the Squared Difference and expand it
    \begin{align}
    (X-Y)^2 &= X^2 + Y^2 -2XY
  \end{align}
  \item Expand on the ideas from Simple Linear Regression
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% Model %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model}

\begin{frame}[fragile]
\begin{itemize}
  \item Theoretical model:
    \begin{align}
    Z &= \beta_0 + \beta_1 X + \beta_2 Y + \beta_3 X^2 + \beta_4 Y^2 + \beta_5 XY + \epsilon
  \end{align}
  \item Fitted model:
    \begin{align}
    \hat{z} &= b_{0} + b_{1}x + b_{2}y + b_3 x^2 + b_4 xy + b_5 y^2 \label{fittedmod} 
    \end{align}
\end{itemize}

We can fit a model like this in $\verb|R|$ using the $\verb|lm()|$ function:
<<lm_ex, echo=TRUE, eval=FALSE>>=
# Z - the response variable
# X - the first explanatory variable
# Y - the second explanatory variable
QuadFit <- lm(z ~ x + y + I(x^2) + I(x*y) + I(y^2), data=d)
@
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% Stationarity Points %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Stationary Points}

\begin{frame}
\frametitle{Stationarity Points}
\begin{itemize}
  \item What are they?
  \begin{itemize}
    \item Points where the slope is zero no matter which direction you take the derivative - local max/min
  \end{itemize}
  \item Values of our explanatory variables provide the ``best'' fit for the response \\
  \item How do you derive the stationary points?
  \begin{enumerate}
    \item Take the derivatives of Equation \ref{fittedmod} with respect to $x$ and $y$
    \item Set the derivatives equal to zero
    \item Solve for $x$ and $y$ in terms of the coefficients to find the stationarity points
    \item Refer to these points as $x_0$ and $y_0$
  \end{enumerate}
\end{itemize}
\end{frame}


\begin{frame}[fragile]
The stationary points are:\\

\begin{align}
x_0 &= \frac{b_2 b_4 - 2 b_5 b_1}{4 b_5 b_3 - b_4^2} \label{eq:xstatpt} \\
y_0 &= \frac{b_1 b_4 + 2 b_2 b_3}{4 b_5 b_3 - b_4^2} \label{eq:ystatpt}
\end{align}

These points represent the values in our predictors that will optimize our response (local minimum or maximum depending on the surface shape).

<<stat_pts_function, echo=FALSE, eval=TRUE>>=
statpts <- function(lm){
  b0 <- as.numeric(coef(lm)[1])
  b1 <- as.numeric(coef(lm)[2])
  b2 <- as.numeric(coef(lm)[3])
  b3 <- as.numeric(coef(lm)[4])
  b4 <- as.numeric(coef(lm)[5])
  b5 <- as.numeric(coef(lm)[6])
  
  # Stationary Pts. using the formulas in Eq 10 & 11
  x0  <- (b2*b4 - 2*b1*b5)/(4*b3*b5 - b4^2)
  y0  <- (b1*b4 - 2*b2*b3)/(4*b3*b5 - b4^2)
  
  # Output
  out <- matrix(c(x0, y0), ncol=2)
  colnames(out) <- c("x0","y0")
  out <- data.frame(x0=x0, y0=y0)
  return(out)
}
@

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Pred Response @ Stationary Pts %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predicted Response at Stationarity Points}

\begin{frame}[fragile]
\frametitle{Predicted Response at Stationarity Points}

We then get the local maximum (or minimum) predicted response ($\hat{z}_0$) by plugging in the Stationary Points to Eq \ref{fittedmod}:
\begin{align}
\hat{z}_0 &= b_{0} + b_{1}x_0 + b_{2}y_0 + b_3 x_0^2 + b_4 x_0 y_0 + b_5 y_0^2
\end{align}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Principal Axes %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Principal Axes}

\begin{frame}[fragile]
\frametitle{Principal Axes}
\begin{itemize}
  \item Measure the amount of ``bend'' in two directions at the stationary points
  \item Make interpretations of the model easier by rotating the axes by removing all the cross-product terms (Ex: PCA)
\end{itemize}


In the Creative Component, 
\begin{itemize}
  \item Brought together a complete picture of how to derive the Principal Axes. 
  \begin{itemize}
    \item Khuri and Cornell (1996) in {\it Response Surfaces: Designs and Analyses} lay out some pieces of how derive them but never completely laid out the complete process
  \end{itemize}
\end{itemize}

\end{frame}


\begin{frame}[fragile]

Basic idea in the derivation of principal axes:
\begin{itemize}
  \item Derive canonical equations to get surface in what is known as canonical form
  \item Transform canonical equations back onto original variables
  \item These are the Principal Axes
\end{itemize}

\begin{align}
\quad        y &= -P_{21}x + P_{20} \label{eq:principalaxes1}
\end{align}

where $P_{20} = y_0 + P_{21} x_0$ and $P_{21} = \frac{b_5 - b_3 - \sqrt{(b_5 - b_3)^2 + b_4^2}}{b_4}$. \\


Using similar algebra we can find the second principal axes:
\begin{align}
\quad        y &= -P_{11}x + P_{10} \label{eq:principalaxes2}
\end{align}
where $P_{10} = y_0 + P_{11} x_0$ and $P_{11} = \frac{b_5 - b_3 + \sqrt{(b_5 - b_3)^2 + b_4^2}}{b_4}$.\\

\end{frame}


\begin{frame}[fragile]

Interpretations:
\begin{enumerate}
  \item In a concave surface:
  \begin{itemize}
    \item First principal axis (Eq \ref{eq:principalaxes1}): lowest downward curvature where two explanatory variables create a maximized surface that decreases the least
    \item Second principal axis (Eq \ref{eq:principalaxes2}): greatest downward curvature where two explanatory variables create the greatest decrease in the response
  \end{itemize}
  \item In a convex surface:
  \begin{itemize}
    \item Flip the interpretations of the two Principal Axes
  \end{itemize}
\end{enumerate}


<<principal_function, echo=FALSE, eval=TRUE>>=
paxis <- function(lm){
  # Takes a quadratic lm function just as the function before and just as before
  # coef(lm) gives a vector of all the parameter estimates in the linear model
  # So here we are grabbing the individual parameter estimates from that vector
  b0 <- as.numeric(coef(lm)[1])
  b1 <- as.numeric(coef(lm)[2])
  b2 <- as.numeric(coef(lm)[3])
  b3 <- as.numeric(coef(lm)[4])
  b4 <- as.numeric(coef(lm)[5])
  b5 <- as.numeric(coef(lm)[6])
  
  # Stationary Pts.
  x0  <- (b2*b4 - 2*b1*b5)/(4*b3*b5 - b4^2)
  y0  <- (b1*b4 - 2*b2*b3)/(4*b3*b5 - b4^2)
  
  # First Principal axis
  p11 <- (b5 - b3 + sqrt((b3 - b5)^2 + b4^2))/b4 #slope
  p10 <- y0 - p11*x0 #intercept
  
  # Second Principal axis
  p21 <- (b5 - b3 - sqrt((b3 - b5)^2 + b4^2))/b4
  p20 <- y0 - p21*x0
  
  # Output
  out <- data.frame(p10=p10, p11=p11, p20=p20, p21=p21)
  return(out)
}
@


<<principal_axes, echo=FALSE, message=FALSE>>=
principal_plot <- function(x){
  require(ggplot2)
  require(reshape2)
  x_temp <- seq(-3,3,length.out= 1000)
  y_p1 <- x$p10 + x$p11*x_temp
  y_p2 <- x$p20 + x$p21*x_temp
  y <- melt(data.frame(y_p1, y_p2))
  temp <- cbind(x = rep(x_temp,2), y)
  ggplot(data=temp, aes(x=x, y=value)) + geom_line() + facet_grid(variable~., scales = "free_y")
}

principal_plot_surface <- function(x, lm){
  require(ggplot2)
  require(reshape2)
  require(gridExtra)
  x_temp <- seq(-3,3,length.out= 1000)
  y_p1 <- x$p10 + x$p11*x_temp
  y_p2 <- x$p20 + x$p21*x_temp
  
  x2 <- x_temp
  y2 <- y_p1
  pred1 <- predict(lm, data.frame(x2,y2,x2^2,x2*y2,y2^2))

  x2 <- x_temp
  y2 <- y_p2
  pred2 <- predict(lm, data.frame(x2,y2,x2^2,x2*y2,y2^2))

  a <- ggplot() + geom_line(aes(x=y_p1, y=pred1)) + labs(x=paste("y = ", round(x$p10,2), " + ", round(x$p11,2), "x"), y="z", title="p1")
  b <- ggplot() + geom_line(aes(x=y_p2, y=pred2)) + labs(x=paste("y = ", round(x$p20,2), " + ", round(x$p21,2), "x"), y="z", title="p2")
  grid.arrange(a, b)
}
@

\end{frame}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Re-creation of Previous Results %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Re-creation of Previous Results}

\begin{frame}[fragile]

An example of this methodology being applied is found in Phillips et al. (2012), {\it Congruence research in behavioral medicine: methodological review and demonstration of alternative methodology}.\\

Interested in seeing what created good situations in which patients followed through with their doctors orders

The variables used in this example are:
\begin{align*}
Z_{1,i} &- \text{patient-reported adherence a month after dr. visit for patient } i \text{: } 0-6 \\
Z_{2,i} &- \text{physician's perceived agreement with patient } i  \text{ on illness treatment: } 0-6 \\
X_{i}   &- \text{physician's rating of patient } i \text{'s health: } 1-5 \\
Y_{i}   &- \text{physician's est of how patient } i \text{ would rate of their own health: } 1-5 \\
\end{align*}

\end{frame}




\begin{frame}[fragile]

Here in Table \ref{tab:Phillipsdf}, some summary statistics are presented of the response and explanatory variables used.

<<Phillips_Data_Setup, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE>>=
d <- read.spss("../Phillips_Data/Congruence PRIM.sav", to.data.frame=TRUE)

###### Data Setup ######
d$pre_curr_health <- as.numeric(d$pre_curr_health)
d$Quant_MD_rate_health <- as.numeric(d$Quant_MD_rate_health)
d$Quant_MD_rate_own <- as.numeric(d$Quant_MD_rate_own)
d$MiddleCentered_precurrhealth <- d$pre_curr_health - 3
d$MiddleCentered_MDratehealth  <- d$Quant_MD_rate_health - 3
d$MiddleCentered_MDrateown     <- d$Quant_MD_rate_own - 3


# Z: patient-reported adherence a month after the dr. visit 
# X: physician's rating of patient's health
# Y: physician's est of how the patient would rate of patient's own health
d$z  <- d$PatientAdh_Avg
d$x <- d$MiddleCentered_MDratehealth
d$y <- d$MiddleCentered_MDrateown


d$w <- d$x
d$w[which(!is.na(d$w))] <- 1 # These are all the points where X < Y 
d$w[which(!is.na(d$w) & d$x > d$y)] <- 0
d$w[which(!is.na(d$w) & d$x == d$y)] <- sample(c(0,1),length(d$w[which(!is.na(d$w) & d$x == d$y)]), replace=TRUE)


# Z: physician perceptions of patient-agreement 
# X: 
# Y: 
d$z2 <- d$PhysicianSharedModels
d$x2 <- d$MiddleCentered_MDratehealth 
d$y2 <- d$MiddleCentered_MDrateown
d$w2 <- d$x2
d$w2[which(!is.na(d$w2))] <- 1 # These are all the points where X < Y 
d$w2[which(!is.na(d$w2) & d$x2 > d$y2)] <- 0
d$w2[which(!is.na(d$w2) & d$x2 == d$y2)] <- sample(c(0,1),length(d$w[which(!is.na(d$w2) & d$x2 == d$y2)]), replace=TRUE)

@

<<Phillips__Model, echo=FALSE, warning=FALSE, message=FALSE>>=
# Z: patient-reported adherence a month after the dr. visit 
# X: physician's rating of patient's health
# Y: physician's est of how the patient would rate of patient's own health

# z = x+y
lm <- lm(z~x+y, data=d)
lmR2 <- summary(lm)$r.squared
lmcoef <- coef(lm)

# z = x-y
lmDiff <- lm(z~I(x-y), data=d)
lmDiffR2 <- summary(lmDiff)$r.squared
lmDiffcoef <- coef(lmDiff)

# z = |x-y|
lmAbs <- lm(z~I(abs(x-y)), data=d)
lmAbsR2 <- summary(lmAbs)$r.squared
lmAbscoef <- coef(lmAbs)

# z = x+y+x^2+xy+y^2
lmSquare <- lm(z~x+y+I(x^2)+I(x*y)+I(y^2), data=d)

# z = x+y+w+wx+wy
lmW <- lm(z~x+y+w+I(w*x)+I(w*y), data=d)
lmWR2 <- summary(lmW)$r.squared
lmWcoef <- coef(lmW)

# z = x+y+w+wx+wy
lmWSquare <- lm(z~x+y+w+I(x^2)+I(x*y)+I(y^2)+I(w*x^2)+I(w*x*y)+I(w*y^2), data=d)



# Z: patient-reported adherence a month after the dr. visit 
# X: 
# Y: 

# z = x+y
lm2 <- lm(z2~x2+y2, data=d)
lm2R2 <- summary(lm2)$r.squared
lm2coef <- coef(lm2)

# z2 = x2-y2
lm2Diff <- lm(z2~I(x2-y2), data=d)
lm2DiffR2 <- summary(lm2Diff)$r.squared
lm2Diffcoef <- coef(lm2Diff)

# z2 = |x2-y2|
lm2Abs <- lm(z2~I(abs(x2-y2)), data=d)
lm2AbsR2 <- summary(lm2Abs)$r.squared
lm2Abscoef <- coef(lm2Abs)

# z2 = x2+y2+x2^2+x2y2+y2^2
lm2Square <- lm(z2~x2+y2+I(x2^2)+I(x2*y2)+I(y2^2), data=d)

# z2 = x2+y2+x2^2+x2y2+y2^2
lm2Cube <- lm(z2~x2+y2+I(x2^3)+I(y2^3)+I(x2^2*y2)+I(x2*y2^2)+I(x2*y2)+I(x2^2)+I(y2^2), data=d)

# z2 = x2+y2+w+wx2+wy2
lm2W <- lm(z2~x2+y2+w+I(w*x2)+I(w*y2), data=d)
lm2WR2 <- summary(lm2W)$r.squared
lm2Wcoef <- coef(lm2W)

# z2 = x2+y2+w+wx2+wy2
lm2WSquare <- lm(z2~x2+y2+w+I(x2^2)+I(x2*y2)+I(y2^2)+I(w*x2^2)+I(w*x2*y2)+I(w*y2^2), data=d)
@

<<Phillips_data_table_exploration, echo=FALSE, results='asis'>>=
Phillipsdf <- data.frame(Z1=c(summary(d$z),n=sum(!is.na(d$z))),
                         Z2=c(summary(d$z2),n=sum(!is.na(d$z2))),
                         X=c(summary(d$x),n=sum(!is.na(d$x))),
                         Y=c(summary(d$y),n=sum(!is.na(d$y)))
)
colnames(Phillipsdf) <- c("$Z_1$","$Z_2$","$X$","$Y$")
print(xtable(t(Phillipsdf),caption = c("Summary statistics of response variables($Z_1$ and $Z_2$) and explanatry variable $X$ and $Y$."), label = "tab:Phillipsdf"), sanitize.rownames.function = function(x) {x})
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Normality Checks}
Before we get into whether or not we should use polynomial regression, they first check some statistics for obvious non-normality:
\begin{enumerate}
  \item High Leverage
  \item Influential Points\\
\end{enumerate}

No points were of major concern but one point consistently had a higher leverage and Cook's D value.

\end{frame}



\begin{frame}[fragile]
\frametitle{Criterion Checks}
The models being used in the criteria are listed below (note that these may have been listed previously but are re-listed here for ease of reading):
\begin{align}
\text{Null Model: } &Z = \beta_0 + \epsilon \label{eq:nullmod1} \\
\text{Diff Model: } &Z = \beta_0 + \beta_1 (X-Y) + \epsilon \label{eq:diffmod1} \\
\text{Uncon Diff Model: } &Z = \beta_0 + \beta_1 X + \beta_2 Y + \epsilon \label{eq:uncondiff1} \\
\text{Abs Diff Model: } &Z = \beta_0 + \beta_1 |X-Y| + \epsilon \label{eq:absdiffmod1} \\
\text{Uncon Abs Diff Model: } &Z = \beta_0 + \beta_1 X + \beta_2 Y + \beta_3 W + \beta_4 WX + \beta_5 WY + \epsilon \label{eq:unconabs1} \\
\text{Fitted Model: } &Z = \beta_0 + \beta_1 X + \beta_2 Y + \beta_3 X^2 + \beta_4 Y^2 + \beta_5 XY + \epsilon \label{eq:polyregmod1}
\end{align}
where $W$ is a dummy variable equal to 0 when $X > Y$, 1 when $X < Y$, and randomly assigned 1 or 0 when $X = Y$, and $\epsilon \sim N(0,\sigma^2_e)$
\end{frame}


\begin{frame}[fragile]
\frametitle{Criterion Checks}

The four criterion are:
\begin{enumerate}
\item{Check whether the unconstrained model (Eq \ref{eq:uncondiff1} and \ref{eq:unconabs1}) explained a significant amount of the variance in the data (compared to Eq \ref{eq:nullmod1}).\\
}
\item{Check whether the regression coefficients for the unconstrained model are in expected directions and whether they are significant to the model.
}
\item{Check whether the unconstrained models (Eq \ref{eq:uncondiff1} and \ref{eq:unconabs1}) is significantly better than their constrained models (Eq \ref{eq:diffmod1} and \ref{eq:absdiffmod1}). \\
}
\item{Check whether a higher order model i.e. polynomial regression (Eq \ref{eq:polyregmod1}) is significantly better than the constrained model (Eq \ref{eq:diffmod1}).
}
\end{enumerate}
\end{frame}



\begin{frame}[fragile]
\frametitle{Criterion Checks}
\begin{enumerate}
 \item First criterion checks the basic linear model assumption i.e. is the model better than a basic mean(intercept) only model (Eq \ref{eq:nullmod})?
 \item Second criterion checks whether the coefficients are lined up in the way we expect them too and whether they add anything to the model
  \item Third criterion checks if there is benefit to using simple polynomial models compared to simpler difference models
 \item Fourth criterion checks whether a simple or complex polynomial model should be used 
\end{enumerate}
\end{frame}


\begin{frame}[fragile]

\begin{table}[!htp]
\caption{This table is a replication of the Table 2 in Phillips et al. (2012) where $* p <0.05$; $** p <0.01$; $*** p <.001$}
\begin{center}
\label{tab:PhillipsTabReplication}
\smallskip\noindent
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccccccccc}
\hline
Outcome                                   & \multicolumn{2}{l}{Constrained Model}          &  & \multicolumn{6}{l}{Unconstrained Model}                                                                                                  &  & $F_c$                                    & $F_h$                                          \\ \cline{2-3} \cline{5-10}
                                          & $(X-Y)$                    & $R^2$               &  & $X$                     & $Y$                     &                       &                       &                       & $R^2$            &  &                                          &                                                \\
\hline
Patient Adherence                         & $\Sexpr{round(lmDiffcoef[2], 2)}^{***}$  & $\Sexpr{round(lmDiffR2,2)}^{***}$  &  & $\Sexpr{round(lmcoef[2],2)}^{**}$   & $\Sexpr{round(lmcoef[3],2)}^{***}$   &                       &                       &                       & $\Sexpr{round(lmR2,2)}^{**}$   &  & $\Sexpr{round(anova(lm, lmDiff)$F[2],2)}$   & $\Sexpr{round(anova(lmSquare, lmDiff)$F[2],2)}$   \\
Physician perception of patient agreement & $\Sexpr{round(lm2Diffcoef[2],2)}$ & $\Sexpr{round(lm2DiffR2,3)}$ &  & $\Sexpr{round(lm2coef[2],2)}$  & $\Sexpr{round(lm2coef[3],2)}^{**}$  &                       &                       &                       & $\Sexpr{round(lm2R2,2)}^{***}$  &  & $\Sexpr{round(anova(lm2, lm2Diff)$F[2],2)}^{***}$ & $\Sexpr{round(anova(lm2Square, lm2Diff)$F[2],2)}^{**}$ \\
\hline
                                          & $|X-Y|$                    & $R^2$               &  & $X$                     & $Y$                     & $W$                     & $WX$                    & $WY$                    & $R^2$            &  &                                          &                                                \\
\hline
Patient Adherence                         & $\Sexpr{round(lmAbscoef[2],2)}^{***}$   & $\Sexpr{round(lmAbsR2,2)}^{***}$   &  & $\Sexpr{round(lmWcoef[2],2)}^{**}$  & $\Sexpr{round(lmWcoef[3],2)}^{**}$  & $\Sexpr{round(lmWcoef[4],2)}$  & $\Sexpr{round(lmWcoef[5],2)}^{*}$  & $\Sexpr{round(lmWcoef[6],2)}$  & $\Sexpr{round(lmWR2,2)}^{**}$  &  & $\Sexpr{round(anova(lmAbs, lmW)$F[2],2)}$   & $\Sexpr{round(anova(lmAbs, lmWSquare)$F[2],2)}^{*}$   \\
Physician perception of patient agreement & $\Sexpr{round(lm2Abscoef[2],2)}^{**}$  & $\Sexpr{round(lm2AbsR2,2)}^{**}$  &  & $\Sexpr{round(lm2Wcoef[2],2)}$ & $\Sexpr{round(lm2Wcoef[3],2)}^{**}$ & $\Sexpr{round(lm2Wcoef[4],2)}$ & $\Sexpr{round(lm2Wcoef[5],2)}^{**}$ & $\Sexpr{round(lm2Wcoef[6],2)}^{*}$ & $\Sexpr{round(lm2WR2,2)}^{***}$ &  & $\Sexpr{round(anova(lm2Abs, lm2W)$F[2],2)}^{***}$ & $\Sexpr{round(anova(lm2Abs, lm2WSquare)$F[2],2)}$ \\
\hline
\end{tabular}}
\end{center}
\end{table}

\end{frame}



\begin{frame}[fragile]
\frametitle{Criterion Checks}
\begin{enumerate}
  \item Criterion 1: Unconstrained model for both outcomes explained a significant amount of variation in the data ($R^2$ of $\Sexpr{round(lmR2,2)}$,$\Sexpr{round(lm2R2,2)}$ for responses $Z_1$ and $Z_2$ respectively). 
  \item Criterion 2: Only satisfied for the outcome dealing with patient adherence(coefficients of -.33 and .39) as the regression coefficients for physician perceptions (.1 and .27) were not going in opposite directions and were not both significant 
  \item Criterion 3: Met for the second outcome i.e. the physician perceptions ($F_c = \Sexpr{round(anova(lm2, lm2Diff)$F[2],2)}$) and not for patient adherence ($F_c = \Sexpr{round(anova(lm, lmDiff)$F[2],2)}$)
  \item Criterion 4: Satisfied for the outcome concerning physician perceptions of patient-agreement ($F_h = \Sexpr{round(anova(lm2Square, lm2Diff)$F[2],2)}$) and not for patient adherence ($F_h = \Sexpr{round(anova(lmSquare, lmDiff)$F[2],2)}$). 
\end{enumerate}

Consider using polynomial regression for the outcome dealing with physician perceptions while it looks as if the model using patient adherence as its outcome will be sufficiently explained using an algebraic difference score model.
\end{frame}


\begin{frame}[fragile]
\frametitle{Criterion Checks}
\begin{itemize}
  \item Repeat above looking at the absolute difference
  \item Conclude the absolute-value difference score model was not an adequate fit for either of the outcome variables
  \item Difference score is an adequate model for the outcome concerning patient adherence 
  \item Polynomial regression would be the best model for the data using the physician perception outcome\\
\end{itemize}
Then they test what kind of polynomial regression:
\begin{itemize}
  \item Quadratic model vs linear model with a p-value of $\Sexpr{anova(lm2, lm2Square)$'Pr(>F)'[2]}$: Quadratic model explains the data better 
  \item Cubic vs quadratic (p-value $\Sexpr{anova(lm2Square, lm2Cube)$'Pr(>F)'[2]}$): Cubic does not explain the data significantly better
\end{itemize}
\end{frame}


\begin{frame}[fragile]

So the fitted model(Eq \ref{eq:polyregmod1}) via OLS:
\begin{align*}
\hat{z} = \Sexpr{round(coef(lm2Square)[1], 2)} + \Sexpr{round(coef(lm2Square)[2], 2)}x + \Sexpr{round(coef(lm2Square)[3], 2)}y \Sexpr{round(coef(lm2Square)[4], 2)} x^2 + \Sexpr{round(coef(lm2Square)[5], 2)} xy \Sexpr{round(coef(lm2Square)[6], 2)} y^2
\end{align*}

Stationary points are at $(\Sexpr{round(statpts(lm2Square)$x0,2)}, \Sexpr{round(statpts(lm2Square)$y0,2)})$\\

The first and second principal axis are Equations \ref{PhillipsPrincipalEq1} and \ref{PhillipsPrincipalEq2}, respectively:
\begin{align}
y = \Sexpr{round(paxis(lm2Square)$p10,2)} + \Sexpr{round(paxis(lm2Square)$p11,2)} x \label{PhillipsPrincipalEq1} \\
y = \Sexpr{round(paxis(lm2Square)$p20,2)} \Sexpr{round(paxis(lm2Square)$p21,2)} x \label{PhillipsPrincipalEq2}
\end{align}
\end{frame}



\begin{frame}[fragile]
The surface slice plots in of both principal axes in Figure \ref{fig:Phillips_Principal_Axes}.
<<Phillips_Principal_Axes, echo=FALSE,warning=FALSE,fig.cap="This Figure shows slices of the response surface taken at the two principal axes.", fig.height=6, fig.width=11, fig.pos='htp'>>=
principal_plot_surface(paxis(lm2Square), lm2Square)
@
\end{frame}



\begin{frame}[fragile]
To look at the predicted surface plot, we use $\verb|Plotly|$. 

We can follow the \href{https://plot.ly/~nulloa1/145/}{link} to the $\verb|Plotly|$ website and look at and manipulate the predicted surface.
\end{frame}



\begin{frame}
\frametitle{Conclusions}
\begin{itemize}
  \item First principal axis is the axis that makes the most sense: it represents the path in which the values of patients’ preferences for shared decision-making and the values of patients’ experiences of shared decision-making along their entire scales that optimize patient satisfaction
  \item When doctor and patient agree with what is going on, the patient is most likely to follow the orders and vice-versa
  \item Second principal axis doesn't have any real interpretations because it lies outside of the scope of the data i.e. it doesn't lie on our predictive surface
\end{itemize}
\end{frame}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% Our Example %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Our Example}

\begin{frame}[fragile]
\frametitle{Our Example}
Overall goal:
\begin{itemize}
  \item Interested in understanding the congruence between survey reports and observer reports\\
\end{itemize}

Background on data:
\begin{itemize}
  \item Interested in understanding the congruence between survey reports and observer reports
  \item Comes from the Iowa Midlife Transitions Project (MTP); this longitudinal study conducted between $1991$ and $2001$ on families from eight counties in rural Iowa. 
  \item Study was designed to look at the effects of the 1960's farm crisis on rural Midwestern families
  \item Families in MTP either participated in the Iowa Youth and Families Project (IFYP) or the Iowa Single Parent Project (ISPP). 
\end{itemize}

\end{frame}


\begin{frame}
IFYP:
\begin{itemize}
  \item Started in 1989 to follow families with at least 2 children and married parents; 
  \item Requirement: one of the children be in seventh grade in '89 while the other sibling was in a four year range in age of the child. 
  \item $78\%$ participation rate of all possible participants in the IFYP
\end{itemize}
  
ISPP:
\begin{itemize}
  \item Started in '91
  \item Focus on recently divorced mothers with a minimum of two children, one of which was a ninth grader in the starting year and the other child was in a 4 year age range of the other child.
  \item $99\%$ participation rate.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Data}

Our focus was on the husband and wife relationship variables. Our variables of interest are divided into two sets:\\


Set 1:
\begin{align*}
Z &: \text{Observed Relationship Quality of couple } i \text{ at Time Wave 0: } 1-9 \\
Y_1 &: \text{Wife}(i)\text{'s Report of Husband's Hostility at Time Wave 0: } 1-5 \\
X_1 &: \text{Husband}(i)\text{'s Report of Husband's Hostility at Time Wave 0: } 1-5 \\
\end{align*}

Set 2:
\begin{align*}
Z &: \text{Observed Relationship Quality of couple } i \text{ at Time Wave 0: } 1-9 \\
Y_2 &: \text{Wife}(i)\text{'s Report of Relationship Instability at Time Wave 0: } 1-4 \\
X_2 &: \text{Husband}(i)\text{'s Report of Relationship Instability at Time Wave 0: } 1-4 \\
\end{align*}
\end{frame}




\begin{frame}[fragile]

Here in Table \ref{tab:Ourdf}, some summary statistics are presented of the response and explanatory variables used.

<<Our_Data_ex, echo=FALSE, message=FALSE>>=
d <- read.csv("../CurrentData/ModData_6_9.csv", header=TRUE)

###### Start Analysis ######
# Using Hostility as a predictor.

x <- d$HHhostW0 - 4
y <- d$WHhostW0 - 4
z <- d$OHrqW0 - 5

w <- x
w[which(!is.na(w))] <- 1 # These are all the points where X < Y 
w[which(!is.na(w) & x > y)] <- 0
w[which(!is.na(w) & x == y)] <- sample(c(0,1),length(w[which(!is.na(w) & x==y)]), replace=TRUE)

# z = (x-y)
lmDiff <- lm(z~I(x-y))
lmDiffR2 <- summary(lmDiff)$r.squared
lmDiffcoef <- coef(lmDiff)

# z = |x-y|
lmAbs <- lm(z ~ I(abs(x-y)))
lmAbsR2 <- summary(lmAbs)$r.squared
lmAbscoef <- coef(lmAbs)

# z = x+y+w+wx+wy
lmW <- lm(z~x+y+w+I(w*x)+I(w*y))
lmWR2 <- summary(lmW)$r.squared
lmWcoef <- coef(lmW)

# z = x+y+w+wx+wy
lmWSquare <- lm(z~x+y+w+I(x^2)+I(x*y)+I(y^2)+I(w*x^2)+I(w*x*y)+I(w*y^2))

# z = x+y
lm <- lm(z~x+y)
lmR2 <- summary(lm)$r.squared
lmcoef <- coef(lm)

# z = x+y+x^2+xy+y^2
lmSquare <- lm(z~x+y+I(x^2)+I(x*y)+I(y^2))

# z = x+y+x^2+xy+y^2+x^3+x^2y+xy^2+y^3
lmCube <- lm(z~x+y+I(x^2)+I(x*y)+I(y^2)+I(x^3)+I((x^2)*y)+I(x*y^2)+I(y^3))

# Using Instability as a predictor.
z <- d$OHrqW0 - 5
x <- d$WWinstH0 - 2.5
y <- d$HHinstW0 - 2.5
w <- x
w[which(!is.na(w))] <- 1 # These are all the points where X < Y 
w[which(!is.na(w) & x > y)] <- 0
w[which(!is.na(w) & x == y)] <- sample(c(0,1),length(w[which(!is.na(w) & x==y)]), replace=TRUE)

# z = (x-y)
lm2Diff <- lm(z ~ I(x-y))
lm2DiffR2 <- summary(lm2Diff)$r.squared
lm2Diffcoef <- coef(lm2Diff)

# z = x+y
lm2 <- lm(z ~ x + y)
lm2R2 <- summary(lm2)$r.squared
lm2coef <- coef(lm2)

# z = |x-y|
lm2Abs <- lm(z ~ I(abs(x-y)))
lm2AbsR2 <- summary(lm2Abs)$r.squared
lm2Abscoef <- coef(lm2Abs)

# z = x+y+x^2+xy+y^2
lm2Square <- lm(z ~ x + y + I(x^2) + I(x*y) + I(y^2))

# z = x+y+x^2+xy+y^2+x^3+x^2y+xy^2+y^3
lm2Cube <- lm(z~x+y+I(x^2)+I(x*y)+I(y^2)+I(x^3)+I((x^2)*y)+I(x*y^2)+I(y^3))

# z = x+y+w+wx+wy
lm2W <- lm(z~x+y+w+I(w*x)+I(w*y))
lm2WR2 <- summary(lm2W)$r.squared
lm2Wcoef <- coef(lm2W)

# z = x+y+w+wx+wy
lm2WSquare <- lm(z~x+y+w+I(x^2)+I(x*y)+I(y^2)+I(w*x^2)+I(w*x*y)+I(w*y^2))
@

<<Our_Model_Check, echo=FALSE, eval=FALSE>>=
# Check the leverage (maybe one point of concern)
lev <- hat(model.matrix(lm1))
plot(lev)

# Basically see the same
cook <- cooks.distance(lm1)
plot(cook,ylab="Cooks distances")

# Nothing of concern. maybe one point.
@


<<our_data_table_exploration,echo=FALSE, results='asis', eval=TRUE>>=
ourdf <- data.frame(Z=c(summary(d$OHrqW0 - 5),n=sum(!is.na(d$OHrqW0))),
                    X1=c(summary(d$HHhostW0 - 4),n=sum(!is.na(d$HHhostW0))),
                    Y1=c(summary(d$WHhostW0 - 4),n=sum(!is.na(d$WHhostW0))),
                    X2=c(summary(d$WWinstH0 - 2.5),n=sum(!is.na(d$WWinstH0))),
                    Y2=c(summary(d$HHinstW0 - 2.5),n=sum(!is.na(d$HHinstW0)))
)
colnames(ourdf) <- c("$Z$","$X_1$","$Y_1$","$X_2$","$Y_2$")
print(xtable(t(ourdf),caption = c("Summary statistics of response variable ($Z$) and explanotry variables ($X_1$,$Y_1$ and $X_2$,$Y_2$)."), label = "tab:Ourdf"), sanitize.rownames.function = function(x) {x})
@



\end{frame}


\begin{frame}[fragile]
\frametitle{Normality Checks}
Just like before we checked some statistics for obvious non-normality:
\begin{enumerate}
  \item High Leverage
  \item Influential Points\\
\end{enumerate}

No points were of major concern.

\end{frame}



\begin{frame}[fragile]
\frametitle{Criterion Checks}
The models being used in the criteria are listed below (note that these may have been listed previously but are re-listed here for ease of reading):
\begin{align}
\text{Null Model: } &Z = \beta_0 + \epsilon \label{eq:nullmod} \\
\text{Diff Model: } &Z = \beta_0 + \beta_1 (X-Y) + \epsilon \label{eq:diffmod} \\
\text{Uncon Diff Model: } &Z = \beta_0 + \beta_1 X + \beta_2 Y + \epsilon \label{eq:uncondiff} \\
\text{Abs Diff Model: } &Z = \beta_0 + \beta_1 |X-Y| + \epsilon \label{eq:absdiffmod} \\
\text{Uncon Abs Diff Model: } &Z = \beta_0 + \beta_1 X + \beta_2 Y + \beta_3 W + \beta_4 WX + \beta_5 WY + \epsilon \label{eq:unconabs} \\
\text{Fitted Model: } &Z = \beta_0 + \beta_1 X + \beta_2 Y + \beta_3 X^2 + \beta_4 Y^2 + \beta_5 XY + \epsilon \label{eq:polyregmod}
\end{align}
where $W$ is a dummy variable equal to 0 when $X > Y$, 1 when $X < Y$, and randomly assigned 1 or 0 when $X = Y$, and $\epsilon \sim N(0,\sigma^2_e)$
\end{frame}


\begin{frame}[fragile]
\frametitle{Criterion Checks}

The four criterion are:
\begin{enumerate}
\item{Check whether the unconstrained model (Eq \ref{eq:uncondiff} and \ref{eq:unconabs}) explained a significant amount of the variance in the data (compared to Eq \ref{eq:nullmod}).\\
}
\item{Check whether the regression coefficients for the unconstrained model are in expected directions and whether they are significant to the model.
}
\item{Check whether the unconstrained models (Eq \ref{eq:uncondiff} and \ref{eq:unconabs}) is significantly better than their constrained models (Eq \ref{eq:diffmod} and \ref{eq:absdiffmod}). \\
}
\item{Check whether a higher order model i.e. polynomial regression (Eq \ref{eq:polyregmod}) is significantly better than the constrained model (Eq \ref{eq:diffmod}).
}
\end{enumerate}
\end{frame}



\begin{frame}[fragile]
\frametitle{Criterion Checks}
\begin{enumerate}
 \item First criterion checks the basic linear model assumption i.e. is the model better than a basic mean(intercept) only model (Eq \ref{eq:nullmod})?
 \item Second criterion checks whether the coefficients are lined up in the way we expect them too and whether they add anything to the model
  \item Third criterion checks if there is benefit to using simple polynomial models compared to simpler difference models
 \item Fourth criterion checks whether a simple or complex polynomial model should be used 
\end{enumerate}
\end{frame}



\begin{frame}[fragile]
\begin{table}[htp]
\caption{This table follows the same format as Table \ref{tab:PhillipsTabReplication}. Here we see that the   where $* p <0.05$; $** p <0.01$; $*** p <.001$}
\begin{center}
\label{tab:OurModelComparison}
\smallskip\noindent
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccccccccc}
\hline
Outcome                                   & \multicolumn{2}{l}{Constrained Model}          &  & \multicolumn{6}{l}{Unconstrained Model}                                                                                                  &  & $F_c$                                    & $F_h$                                          \\ \cline{2-3} \cline{5-10}
                                          & $(X-Y)$                    & $R^2$               &  & $X$                     & $Y$                     &                       &                       &                       & $R^2$            &  &                                          &                                                \\
\hline
Relationship Quality w/ Hostility                        & $\Sexpr{round(lmDiffcoef[2],2)}$  & $\Sexpr{round(lmDiffR2,2)}$  &  & $\Sexpr{round(lmcoef[2],2)}^{***}$   & $\Sexpr{round(lmcoef[3],2)}^{***}$   &                       &                       &                       & $\Sexpr{round(lmR2,2)}^{***}$   &  & $\Sexpr{round(anova(lm, lmDiff)$F[2],2)}^{***}$   & $\Sexpr{round(anova(lmSquare, lmDiff)$F[2],2)}^{***}$   \\
Relationship Quality w/ Instability & $\Sexpr{round(lm2Diffcoef[2],2)}$ & $\Sexpr{round(lm2DiffR2,2)}$ &  & $\Sexpr{round(lm2coef[2],2)}^{*}$  & $\Sexpr{round(lm2coef[2],2)}^{***}$  &                       &                       &                       & $\Sexpr{round(lm2R2,2)}^{***}$  &  & $\Sexpr{round(anova(lm2, lm2Diff)$F[2],2)}^{***}$ & $\Sexpr{round(anova(lm2Square, lm2Diff)$F[2],2)}^{***}$ \\
\hline
                                          & $|X-Y|$                    & $R^2$               &  & $X$                     & $Y$                     & $W$                     & $WX$                    & $WY$                    & $R^2$            &  &                                          &                                                \\
\hline
Relationship Quality w/ Hostility                      & $\Sexpr{round(lmAbscoef[2],2)}^{**}$   & $\Sexpr{round(lmAbsR2,2)}^{**}$   &  & $\Sexpr{round(lmWcoef[2],2)}^{*}$  & $\Sexpr{round(lmWcoef[3],2)}^{**}$  & $\Sexpr{round(lmWcoef[4],2)}$  & $\Sexpr{round(lmWcoef[5],2)}$  & $\Sexpr{round(lmWcoef[6],2)}$  & $\Sexpr{round(lmWR2,2)}^{***}$  &  & $\Sexpr{round(anova(lmAbs, lmW)$F[2],2)}^{***}$   & $\Sexpr{round(anova(lmAbs, lmWSquare)$F[2],2)}^{***}$   \\
Relationship Quality w/ Instability & $\Sexpr{round(lm2Abscoef[2],2)}^{***}$  & $\Sexpr{round(lm2AbsR2,2)}^{***}$  &  & $\Sexpr{round(lm2Wcoef[2],2)}$ & $\Sexpr{round(lm2Wcoef[3],2)}^{***}$ & $\Sexpr{round(lm2Wcoef[4],2)}$ & $\Sexpr{round(lm2Wcoef[5],2)}$ & $\Sexpr{round(lm2Wcoef[6],2)}$ & $\Sexpr{round(lm2WR2,2)}^{***}$ &  & $\Sexpr{round(anova(lm2Abs, lm2W)$F[2],2)}^{***}$ & $\Sexpr{round(anova(lm2Abs, lm2WSquare)$F[2],2)}^{***}$ \\
\hline
\end{tabular}}
\end{center}
\end{table}

\end{frame}



\begin{frame}[fragile]
\frametitle{Criterion Checks}
\begin{enumerate}
  \item Criterion 1: Unconstrained model explained a significant amount of variation in the data ($R^2 = .22$, pval of $\Sexpr{anova(lmDiff, lm)$Pr[2]}$ and $R^2 = .11$, pval of $\Sexpr{anova(lm2Diff, lm2)$Pr[2]}$, respectively) in both sets
  \item Criterion 2: Regression coefficients for both models were consistent with each other which is what we would expect. For both of the unconstrained algebraic models, the regression coefficients are significant, but the regression coefficients for the unconstrained absolute difference models were not all significant
  \item Criterion 3 was both met by both examples/models ($\Sexpr{round(anova(lm, lmDiff)$F[2],2)}^{***}$, $\Sexpr{round(anova(lm2, lm2Diff)$F[2],2)}^{***}$
  \item Criterion 4: was both met by both examples/models $\Sexpr{round(anova(lmSquare, lmDiff)$F[2],2)}^{***}$, $\Sexpr{round(anova(lm2Square, lm2Diff)$F[2],2)}^{***}$)
\end{enumerate}

Both models suggest using something greater than a simple difference model
\end{frame}


\begin{frame}[fragile]
\frametitle{Criterion Checks}
\begin{itemize}
  \item Repeat above looking at the absolute difference
  \item Conclude the absolute-value difference score model was not an adequate fit for the model considering Relationship Quality with Instability as a predictor
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Criterion Checks}
What kind of polynomial regression:
\begin{itemize}
\item Model with Hostility as a predictor: Use a linear model between to the two predictors and the response (p-val $\Sexpr{anova(lm, lmSquare)$Pr[2]}$ i.e. $H0: \beta_3 = \beta_4 = \beta_5 = 0$  in Eq \ref{eq:polyregmod})
  \item Model with Instability as a predictor: has moderate evidence of using a quadratic model for the relationship (p-val $\Sexpr{anova(lm2, lm2Square)$Pr[2]}$ i.e. $H0: \beta_3 = \beta_4 = \beta_5 = 0$  in Eq \ref{eq:polyregmod}) and no evidence for using a cubic model on the relationship (p-val $\Sexpr{anova(lm2Cube, lm2Square)$Pr[2]}$)
\end{itemize}
\end{frame}



\begin{frame}[fragile]

The fitted quadratic polynomial using perceived relationship instability as a predictor for relationship quality is:
\begin{align}
\hat{z} &= \Sexpr{round(coef(lm2Square)[1], 2)} + \Sexpr{round(coef(lm2Square)[2], 2)}x + \Sexpr{round(coef(lm2Square)[3], 2)}y + \Sexpr{round(coef(lm2Square)[4], 2)} x^2 + \Sexpr{round(coef(lm2Square)[5], 2)} xy + \Sexpr{round(coef(lm2Square)[6], 2)} y^2 
\end{align} \\

The estimates of the model and their standard errors in Table \ref{tab:Ourdf}
<<model_est, echo=FALSE, results='asis'>>=
MEdf <- t(summary(lm2Square)$coefficients[,c(1,2,4)])
colnames(MEdf) <- c("$b_0$","$b_1$","$b_2$","$b_3$","$b_4$","$b_5$")
rownames(MEdf) <- c("Estimate","Std. Error","Pr$(>|t|)$")
print(xtable(MEdf,caption = c("Model estimates and SE for polynomial regression model using relationship instability ratings as a explanatory variables."), label = "tab:Ourdf"), sanitize.text.function = function(x){x})
@
\end{frame}



\begin{frame}[fragile]

Stationary points are at $(\Sexpr{round(statpts(lm2Square)$x0,2)}, \Sexpr{round(statpts(lm2Square)$y0,2)})$\\

Likewise we can look at the principal axes:
\begin{align}
y &= \Sexpr{round(paxis(lm2Square)$p10,2)} + \Sexpr{round(paxis(lm2Square)$p11,2)} x \label{eq:Our_pa1}\\
y &= \Sexpr{round(paxis(lm2Square)$p20,2)} \Sexpr{round(paxis(lm2Square)$p21,2)} x \label{eq:Our_pa2}
\end{align}

\end{frame}



\begin{frame}[fragile]

The surface slice plots of both principal axes are in Figure \ref{fig:Our_Principal_Axes}.
<<Our_Principal_Axes, echo=FALSE,warning=FALSE,fig.cap="This Figure shows slices of the response surface taken at the two principal axes.", fig.height=6, fig.width=11, fig.pos='htp'>>=
require(ggplot2)
require(reshape2)
require(gridExtra)
x_temp <- seq(-3,3,length.out= 1000)
y_p1 <- paxis(lm2Square)$p10 + paxis(lm2Square)$p11*x_temp
y_p2 <- paxis(lm2Square)$p20 + paxis(lm2Square)$p21*x_temp

x <- x_temp
y <- y_p1
pred1 <- predict(lm2Square, data.frame(x,y,x^2,x*y,y^2))
x <- x_temp
y <- y_p2
pred2 <- predict(lm2Square, data.frame(x,y,x^2,x*y,y^2))

a <- ggplot() + geom_line(aes(x=y_p1, y=pred1)) + labs(x=paste("y = ", round(paxis(lm2Square)$p10,2), " + ", round(paxis(lm2Square)$p11,2), "x"), y="z", title="p1")
b <- ggplot() + geom_line(aes(x=y_p2, y=pred2)) + labs(x=paste("y = ", round(paxis(lm2Square)$p20,2), " ", round(paxis(lm2Square)$p21,2), "x"), y="z", title="p2")
grid.arrange(a, b)

@

\end{frame}


\begin{frame}[fragile]
To look at the predicted surface plot, we use $\verb|Plotly|$. 

We can follow the \href{https://plot.ly/~nulloa1/147/}{link} to the $\verb|Plotly|$ website and look at and manipulate the predicted surface.
\end{frame}



\begin{frame}
\frametitle{Conclusions}
\begin{itemize}
  \item Convex surface so interpretations of the principal axis are flipped; 
  \begin{itemize}
    \item First principal axis, Equation \ref{eq:Our_pa1}, represents the line in the surface where relationship quality decreases the most i.e. high level of perceived relationship instability by the wife
    \item Second principal axis, Equation \ref{eq:Our_pa2}, is where the surface has the lowest downward curvature in relationship quality i.e. when RQ does not decrease that much when the wife's perceived relationship instability hangs around 0(2)
  \end{itemize}
  \item First principal axis lies out of the bounds of data, but the second is good
  \item Most integral to relationship quality is the wife's perception of instability
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Conclusions}

Pros:
\begin{itemize}
  \item Easily visualize the realm of possible combinations of the predictor variables and their effect on the predicted response
  \item Helps identify the underlying mechanism by seeing how different predictors affect the surface
\end{itemize}

Cons:
\begin{itemize}
  \item Not super intuitive interpretations of model components
  \item Issues with data greatly affect inferences
  \item Figure \ref{fig:data_skew_plot} and \ref{fig:data_dist_plot} shows the predictor variables are very left skewed
\end{itemize}
\end{frame}



\begin{frame}[fragile]
\frametitle{Conclusions}
<<data_skew_plot,message=FALSE, echo=FALSE,warning=FALSE,fig.cap="This Figure shows the histograms of our two predictor variables plotted on top of each other to show skewness.", fig.height=5, fig.width=11, fig.pos='htp'>>=
skewcomb <- data.frame(d$WWinstH0,d$HHinstW0)
skewcomb <- data.frame(melt(skewcomb, by="d.OHrqW0"), rep(d$OHrqW0,2))
levels(skewcomb$variable) <- c("Wife", "Husband")
names(skewcomb) <- c("Reporter","Instability","ObservedRQ")
ggplot(skewcomb, aes(x=Instability, fill=Reporter)) + geom_histogram(alpha=0.5, position="identity")
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Conclusions}
<<data_dist_plot, message=FALSE, echo=FALSE,warning=FALSE,fig.cap="This Figure shows a scatterplot of Instability against Observed Relationship Quality colored by who reported the instability rating.", fig.height=5, fig.width=11, fig.pos='htp'>>=
ggplot(skewcomb, aes(x=Instability, y=ObservedRQ, color=Reporter)) + geom_point(alpha=0.5) + geom_jitter()
@
\end{frame}


%------------------------------------%
%           Bibliography             %
%------------------------------------%

\section{References}
\begin{frame}
\frametitle{References}
\nocite{*}
\bibliographystyle{amsalpha}
\bibliography{writeup_references.bib}
\end{frame}






\section{Thank You}
\begin{frame}
Thank you to:
\begin{itemize}
  \item Dr. Lorenz
  \item Breanne Ulloa
  \item Dr. Phillips $\&$ Dr. Koehler
\end{itemize}

\end{frame}


\end{document}